"""
Database Backup and Restore Router for PKMS Backend

Provides database backup, restore, and management endpoints
with proper authentication and security controls.

WAL (Write-Ahead Log) Management Strategy:
=========================================

## Automatic WAL Checkpoints (SQLite Default Behavior):
1. **Size-based**: WAL auto-checkpoints when reaching 1000 pages (~4MB)
2. **Connection close**: WAL checkpoints when last connection closes
3. **Manual triggers**: PRAGMA wal_checkpoint commands
4. **Transaction commits**: Some commits may trigger partial checkpoints

## Our Backup Strategy:
1. **Manual Checkpoint**: We force WAL checkpoint before backup to ensure ALL data inclusion
2. **Data Guarantee**: No data loss - recent changes always captured
3. **Industry Standard**: Follows SQLite's official backup recommendations

## Why This Matters:
- Without checkpoint: Backup might miss recent changes still in WAL
- With checkpoint: Backup is guaranteed complete and up-to-date
- Performance: Checkpoint is fast (milliseconds) vs incomplete backup risk

## WAL File Lifecycle:
- WAL grows with each transaction until checkpoint
- After checkpoint: WAL is cleared/reset
- SHM tracks which pages are in WAL vs main DB
- Both WAL and SHM are recreated as needed
"""

from fastapi import APIRouter, Depends, HTTPException, Query, Form
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
from datetime import datetime, timezone, timedelta
import os
from pathlib import Path
import shutil
import asyncio
import logging

# Nepal Standard Time offset
NEPAL_TZ = timezone(timedelta(hours=5, minutes=45))

# Set up logger
logger = logging.getLogger(__name__)

from ..database import get_db
from ..auth.dependencies import get_current_user
from ..models.user import User

router = APIRouter(tags=["backup"])

@router.post("/create")
async def create_database_backup(
    backup_method: str = Form("checkpoint", description="Backup method: 'checkpoint' (recommended), 'all_files', or 'vacuum'"),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Create a timestamped backup of the database using industry best practices."""
    try:
        # Generate timestamp for backup filename using Nepal time
        nepal_time = datetime.now(NEPAL_TZ)
        timestamp = nepal_time.strftime("%Y%m%d_%H%M%S")
        backup_dir = Path("/app/PKMS_Data/backups")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        if backup_method == "all_files":
            # Method 1: Backup all 3 files (complete snapshot approach)
            backup_base = f"pkm_metadata_full_{timestamp}"
            backup_files = []
            
            files_to_backup = [
                ("/app/data/pkm_metadata.db", f"{backup_base}.db"),
                ("/app/data/pkm_metadata.db-wal", f"{backup_base}.db-wal"), 
                ("/app/data/pkm_metadata.db-shm", f"{backup_base}.db-shm")
            ]
            
            total_size = 0
            for source_path, backup_filename in files_to_backup:
                if os.path.exists(source_path):
                    dest_path = f"/app/PKMS_Data/backups/{backup_filename}"
                    await asyncio.to_thread(shutil.copy, source_path, dest_path)
                    backup_files.append(backup_filename)
                    file_size = Path(dest_path).stat().st_size
                    total_size += file_size
            
            return {
                "status": "success",
                "method": "all_files",
                "message": "Complete database snapshot backup created (includes WAL/SHM files)",
                "backup_files": backup_files,
                "total_files": len(backup_files),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 4),
                "timestamp": nepal_time.isoformat(),
                "created_by": current_user.username,
                "restore_note": "Use the .db file for restore, but keep all files together",
                "industry_practice": "Complete snapshot - guarantees no data loss"
            }
            
        elif backup_method == "vacuum":
            # Method 2: VACUUM INTO (creates clean, optimized copy)
            backup_filename = f"pkm_metadata_vacuum_{timestamp}.db"
            backup_path = f"/app/PKMS_Data/backups/{backup_filename}"
            
            try:
                # VACUUM INTO automatically handles WAL consolidation
                await db.execute(text(f"VACUUM INTO '{backup_path}'"))
                await db.commit()
                
                file_size = Path(backup_path).stat().st_size
                
                return {
                    "status": "success", 
                    "method": "vacuum",
                    "message": "Optimized database backup created with VACUUM INTO",
                    "backup_filename": backup_filename,
                    "file_size_bytes": file_size,
                    "file_size_mb": round(file_size / (1024 * 1024), 4),
                    "timestamp": nepal_time.isoformat(),
                    "created_by": current_user.username,
                    "advantages": "Optimized, defragmented, includes all WAL data",
                    "industry_practice": "VACUUM INTO - SQLite's recommended backup method"
                }
                
            except Exception as vacuum_error:
                raise Exception(f"VACUUM INTO failed: {vacuum_error}")
        
        else:  # backup_method == "checkpoint" (RECOMMENDED)
            # Method 3: Industry Best Practice - Checkpoint WAL then backup main DB
            backup_filename = f"pkm_metadata_backup_{timestamp}.db"
            
            try:
                # STEP 1: Checkpoint WAL to ensure ALL recent changes are in main DB
                logger.info("Checkpointing WAL to consolidate all changes...")
                checkpoint_result = await db.execute(text("PRAGMA wal_checkpoint(FULL)"))
                checkpoint_info = checkpoint_result.fetchone()
                await db.commit()

                # STEP 2: Now backup the main DB file (contains ALL data)
                logger.info("BACKUP: Backing up consolidated database file...")
                source_path = "/app/data/pkm_metadata.db"
                backup_path = f"/app/PKMS_Data/backups/{backup_filename}"
                
                await asyncio.to_thread(shutil.copy, source_path, backup_path)
                
                file_size = Path(backup_path).stat().st_size
                
                return {
                    "status": "success",
                    "method": "checkpoint_backup", 
                    "message": "Industry-standard backup completed: WAL checkpointed + main DB backed up",
                    "backup_filename": backup_filename,
                    "file_size_bytes": file_size,
                    "file_size_mb": round(file_size / (1024 * 1024), 4),
                    "checkpoint_info": {
                        "wal_pages_moved": checkpoint_info[1] if checkpoint_info else "unknown",
                        "wal_checkpoints": checkpoint_info[2] if checkpoint_info else "unknown"
                    },
                    "timestamp": nepal_time.isoformat(),
                    "created_by": current_user.username,
                    "data_guarantee": "All recent changes included via WAL checkpoint",
                    "industry_practice": "RECOMMENDED - Checkpoint first, then backup main DB"
                }
                    
            except Exception as checkpoint_error:
                # Fallback: If checkpoint fails, try VACUUM INTO
                logger.warning(f"Checkpoint failed ({checkpoint_error}), falling back to VACUUM INTO...")
                
                backup_filename = f"pkm_metadata_fallback_{timestamp}.db"
                backup_path = f"/app/PKMS_Data/backups/{backup_filename}"
                
                await db.execute(text(f"VACUUM INTO '{backup_path}'"))
                await db.commit()
                
                file_size = Path(backup_path).stat().st_size
                
                return {
                    "status": "success",
                    "method": "fallback_vacuum",
                    "message": "Backup completed using VACUUM INTO fallback method",
                    "backup_filename": backup_filename,
                    "file_size_bytes": file_size,
                    "file_size_mb": round(file_size / (1024 * 1024), 4),
                    "timestamp": nepal_time.isoformat(),
                    "created_by": current_user.username,
                    "fallback_reason": str(checkpoint_error),
                    "industry_practice": "Fallback method - still guarantees complete data"
                }
            
    except Exception as e:
        return {
            "status": "error",
            "message": "Backup operation failed",
            "error": str(e),
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }

@router.get("/list")
async def list_database_backups(
    current_user: User = Depends(get_current_user)
):
    """List all available database backup files with metadata."""
    try:
        backup_dir = Path("/app/PKMS_Data/backups")
        
        if not backup_dir.exists():
            return {
                "status": "no_backups",
                "message": "No backup directory found",
                "backups": [],
                "backup_count": 0,
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }
        
        # Find all .db files in backup directory
        backup_files = list(backup_dir.glob("*.db"))
        backups = []
        
        for backup_file in sorted(backup_files, key=lambda x: x.stat().st_mtime, reverse=True):
            try:
                stat = backup_file.stat()
                backups.append({
                    "filename": backup_file.name,
                    # Omit absolute server path to avoid disclosure
                    # "full_path": str(backup_file),
                    "relative_path": f"PKMS_Data/backups/{backup_file.name}",
                    "file_size_bytes": stat.st_size,
                    "file_size_kb": round(stat.st_size / 1024, 2),
                    "file_size_mb": round(stat.st_size / (1024 * 1024), 4),
                    "created_at": datetime.fromtimestamp(stat.st_ctime, NEPAL_TZ).isoformat(),
                    "modified_at": datetime.fromtimestamp(stat.st_mtime, NEPAL_TZ).isoformat(),
                    "is_recent": (datetime.now(NEPAL_TZ).timestamp() - stat.st_mtime) < (24 * 3600)  # Within 24 hours
                })
            except Exception:
                # Skip files that can't be read
                continue
        
        return {
            "status": "success" if backups else "no_backups",
            "message": f"Found {len(backups)} backup file(s)" if backups else "No backup files found",
            "backups": backups,
            "backup_count": len(backups),
            "backup_directory": str(backup_dir),
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": "Failed to list backup files",
            "error": str(e),
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }

@router.post("/restore")
async def restore_database_backup(
    backup_filename: str = Form(..., description="Name of backup file to restore"),
    confirm_restore: bool = Form(False, description="Confirmation flag for destructive operation"),
    current_user: User = Depends(get_current_user)
):
    """Restore database from a backup file. WARNING: This replaces the current database!"""
    try:
        if not confirm_restore:
            return {
                "status": "confirmation_required",
                "message": "Database restore requires confirmation. Set confirm_restore=true to proceed.",
                "warning": "This operation will REPLACE the current database with the backup!",
                "backup_filename": backup_filename,
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }

        # Security check: only allow .db files in backups directory
        base_dir = Path("/app/PKMS_Data/backups").resolve()
        name_only = Path(backup_filename).name
        candidate = (base_dir / name_only).resolve()
        if not name_only.endswith('.db') or base_dir not in candidate.parents or candidate.parent != base_dir:
            return {
                "status": "error",
                "message": "Invalid backup filename. Only .db files in backups directory are allowed.",
                "backup_filename": backup_filename,
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }

        # Validate backup file exists
        backup_path = Path(f"/app/PKMS_Data/backups/{backup_filename}")
        if not backup_path.exists():
            return {
                "status": "error",
                "message": "Backup file not found",
                "backup_filename": backup_filename,
                "looked_for": str(backup_path),
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }
        
        # Get backup file info before restore
        backup_stat = backup_path.stat()
        backup_info = {
            "filename": backup_filename,
            "size_bytes": backup_stat.st_size,
            "size_mb": round(backup_stat.st_size / (1024 * 1024), 4),
            "created_at": datetime.fromtimestamp(backup_stat.st_ctime, NEPAL_TZ).isoformat()
        }
        
        # Restore database from backup using direct filesystem operations
        # Copy the backup file to the Docker volume location
        await asyncio.to_thread(shutil.copy, backup_path, "/app/data/pkm_metadata.db")
        
        return {
            "status": "success",
            "message": "Database restored successfully from backup",
            "backup_info": backup_info,
            "warning": "Application restart recommended to ensure clean state",
            "restored_by": current_user.username,
            "timestamp": datetime.now(NEPAL_TZ).isoformat(),
            "note": "All active sessions may need to be refreshed"
        }
            
    except Exception as e:
        return {
            "status": "error",
            "message": "Restore operation failed",
            "error": str(e),
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }

@router.delete("/delete/{backup_filename}")
async def delete_database_backup(
    backup_filename: str,
    confirm_delete: bool = Query(False, description="Confirmation flag for file deletion"),
    current_user: User = Depends(get_current_user)
):
    """Delete a specific backup file. Use with caution!"""
    try:
        if not confirm_delete:
            return {
                "status": "confirmation_required",
                "message": "Backup deletion requires confirmation. Add ?confirm_delete=true to proceed.",
                "backup_filename": backup_filename,
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }
        
        # Security check: only allow .db files in backups directory
        base_dir = Path("/app/PKMS_Data/backups").resolve()
        name_only = Path(backup_filename).name
        candidate = (base_dir / name_only).resolve()
        if not name_only.endswith('.db') or base_dir not in candidate.parents or candidate.parent != base_dir:
            return {
                "status": "error",
                "message": "Invalid backup filename. Only .db files in backups directory are allowed.",
                "backup_filename": backup_filename,
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }
        
        # Validate backup file exists
        backup_path = Path(f"/app/PKMS_Data/backups/{backup_filename}")
        if not backup_path.exists():
            return {
                "status": "error",
                "message": "Backup file not found",
                "backup_filename": backup_filename,
                "timestamp": datetime.now(NEPAL_TZ).isoformat()
            }
        
        # Get file info before deletion
        backup_stat = backup_path.stat()
        backup_info = {
            "filename": backup_filename,
            "size_bytes": backup_stat.st_size,
            "size_mb": round(backup_stat.st_size / (1024 * 1024), 4),
            "created_at": datetime.fromtimestamp(backup_stat.st_ctime, NEPAL_TZ).isoformat()
        }
        
        # Delete the backup file
        backup_path.unlink()
        
        return {
            "status": "success",
            "message": "Backup file deleted successfully",
            "deleted_backup": backup_info,
            "deleted_by": current_user.username,
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": "Failed to delete backup file",
            "error": str(e),
            "backup_filename": backup_filename,
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }

@router.get("/info")
async def get_backup_info(
    current_user: User = Depends(get_current_user)
):
    """Get information about the backup system and current status."""
    try:
        backup_dir = Path("/app/PKMS_Data/backups")
        
        # Count backup files
        backup_count = 0
        total_size = 0
        if backup_dir.exists():
            backup_files = list(backup_dir.glob("*.db"))
            backup_count = len(backup_files)
            total_size = sum(f.stat().st_size for f in backup_files)
        
        return {
            "status": "success",
            "backup_system": {
                "backup_directory": str(backup_dir),
                "directory_exists": backup_dir.exists(),
                "backup_count": backup_count,
                "total_backup_size_bytes": total_size,
                "total_backup_size_mb": round(total_size / (1024 * 1024), 4),
                "database_location": "Docker volume (pkms_db_data)",
                "backup_location": "Windows filesystem (accessible)",
                "file_types_backed_up": ["Database metadata", "User accounts", "Settings"],
                "file_types_not_backed_up": ["Documents", "Images", "Diary content", "Archives"]
            },
            "file_storage_info": {
                "database": "Docker volume (requires backup to access)",
                "user_content": "Windows filesystem (directly accessible)",
                "content_folders": [
                    "PKMS_Data/assets/ - Documents and images",
                    "PKMS_Data/secure/ - Encrypted diary content",
                    "PKMS_Data/archive/ - Archive data",
                    "PKMS_Data/exports/ - Export files"
                ]
            },
            "current_limitations": [
                "Docker access needed for backup/restore operations",
                "Manual Docker socket configuration required",
                "Backup/restore operations temporarily disabled"
            ],
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": "Failed to get backup information",
            "error": str(e),
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        } 

@router.get("/wal-status")
async def get_wal_status(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get current WAL file status and size information."""
    try:
        nepal_time = datetime.now(NEPAL_TZ)
        
        # Check WAL file sizes
        db_path = "/app/data/pkm_metadata.db"
        wal_path = "/app/data/pkm_metadata.db-wal"
        shm_path = "/app/data/pkm_metadata.db-shm"
        
        file_info = {}
        total_size = 0
        
        for name, path in [("main_db", db_path), ("wal", wal_path), ("shm", shm_path)]:
            if os.path.exists(path):
                size = os.path.getsize(path)
                file_info[name] = {
                    "exists": True,
                    "size_bytes": size,
                    "size_kb": round(size / 1024, 2),
                    "size_mb": round(size / (1024 * 1024), 4)
                }
                total_size += size
            else:
                file_info[name] = {"exists": False, "size_bytes": 0}
        
        # Get WAL information from SQLite
        try:
            wal_info_result = await db.execute(text("PRAGMA wal_checkpoint"))
            wal_info = wal_info_result.fetchone()
            
            pragma_result = await db.execute(text("PRAGMA journal_mode"))
            journal_mode = pragma_result.scalar()
            
            page_count_result = await db.execute(text("PRAGMA page_count"))
            page_count = page_count_result.scalar()
            
            page_size_result = await db.execute(text("PRAGMA page_size"))
            page_size = page_size_result.scalar()
            
        except Exception:
            wal_info = None
            journal_mode = "unknown"
            page_count = 0
            page_size = 4096
        
        # Calculate thresholds
        auto_checkpoint_threshold = 1000 * page_size  # 1000 pages
        wal_size = file_info["wal"]["size_bytes"]
        wal_percentage = (wal_size / auto_checkpoint_threshold * 100) if auto_checkpoint_threshold > 0 else 0
        
        # Determine status and recommendations
        if wal_size == 0:
            status = "clean"
            recommendation = "WAL is clean - no action needed"
            color = "green"
        elif wal_size < (auto_checkpoint_threshold * 0.5):
            status = "normal"
            recommendation = "WAL size is normal - no action needed"
            color = "green"
        elif wal_size < (auto_checkpoint_threshold * 0.8):
            status = "growing"
            recommendation = "WAL is growing but still within normal range"
            color = "yellow"
        elif wal_size < auto_checkpoint_threshold:
            status = "approaching_limit"
            recommendation = "WAL approaching auto-checkpoint threshold"
            color = "orange"
        else:
            status = "should_auto_checkpoint"
            recommendation = "WAL exceeds threshold - should auto-checkpoint soon"
            color = "red"
        
        return {
            "status": "success",
            "timestamp": nepal_time.isoformat(),
            "database_mode": journal_mode,
            "files": file_info,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 4),
            "wal_analysis": {
                "current_size_bytes": wal_size,
                "current_size_mb": round(wal_size / (1024 * 1024), 4),
                "auto_checkpoint_threshold_bytes": auto_checkpoint_threshold,
                "auto_checkpoint_threshold_mb": round(auto_checkpoint_threshold / (1024 * 1024), 4),
                "percentage_of_threshold": round(wal_percentage, 2),
                "status": status,
                "recommendation": recommendation,
                "status_color": color
            },
            "database_info": {
                "page_count": page_count,
                "page_size_bytes": page_size,
                "estimated_db_size_mb": round((page_count * page_size) / (1024 * 1024), 4)
            },
            "performance_note": "SQLite automatically optimizes WAL performance. Manual checkpoint rarely needed."
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": "Failed to get WAL status",
            "error": str(e),
            "timestamp": datetime.now(NEPAL_TZ).isoformat()
        }

@router.post("/manual-checkpoint")
async def manual_wal_checkpoint(
    checkpoint_mode: str = Form("FULL", description="Checkpoint mode: PASSIVE, FULL, or RESTART"),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Manually trigger WAL checkpoint - Advanced feature for power users.
    
    Checkpoint Modes:
    - PASSIVE: Non-blocking, only if no readers
    - FULL: Block until complete (recommended) 
    - RESTART: FULL + restart WAL from beginning
    """
    try:
        nepal_time = datetime.now(NEPAL_TZ)
        
        # Validate checkpoint mode
        valid_modes = ["PASSIVE", "FULL", "RESTART"]
        if checkpoint_mode not in valid_modes:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid checkpoint mode. Must be one of: {', '.join(valid_modes)}"
            )
        
        # Get WAL size before checkpoint
        wal_path = "/app/data/pkm_metadata.db-wal"
        wal_size_before = os.path.getsize(wal_path) if os.path.exists(wal_path) else 0
        
        # Execute checkpoint
        logger.info(f"Manual WAL checkpoint ({checkpoint_mode}) initiated by {current_user.username}")
        checkpoint_result = await db.execute(text(f"PRAGMA wal_checkpoint({checkpoint_mode})"))
        checkpoint_info = checkpoint_result.fetchone()
        await db.commit()
        
        # Get WAL size after checkpoint
        wal_size_after = os.path.getsize(wal_path) if os.path.exists(wal_path) else 0
        data_moved = wal_size_before - wal_size_after
        
        return {
            "status": "success",
            "message": f"Manual WAL checkpoint ({checkpoint_mode}) completed successfully",
            "checkpoint_mode": checkpoint_mode,
            "timestamp": nepal_time.isoformat(),
            "executed_by": current_user.username,
            "checkpoint_result": {
                "busy": checkpoint_info[0] if checkpoint_info else 0,
                "log_pages": checkpoint_info[1] if checkpoint_info else 0, 
                "checkpointed_pages": checkpoint_info[2] if checkpoint_info else 0
            },
            "wal_size_change": {
                "before_bytes": wal_size_before,
                "after_bytes": wal_size_after,
                "data_moved_bytes": data_moved,
                "data_moved_mb": round(data_moved / (1024 * 1024), 4)
            },
            "performance_note": "This was a manual checkpoint. SQLite normally handles this automatically.",
            "recommendation": "Manual checkpoints are rarely needed unless you have specific performance requirements."
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": "Manual checkpoint failed",
            "error": str(e),
            "timestamp": nepal_time.isoformat()
        } 