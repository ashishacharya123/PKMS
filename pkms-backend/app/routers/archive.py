"""
Archive Router
Handles hierarchical file and folder organization with enhanced security
"""

from fastapi import APIRouter, Depends, HTTPException, status, Query, Form, File, UploadFile, Request, Response
from fastapi.responses import FileResponse, StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, desc, delete, text
from sqlalchemy.orm import selectinload
from app.schemas.archive import (
    FolderCreate,
    FolderUpdate,
    ItemUpdate,
    FolderResponse,
    ItemResponse,
    ItemSummary,
    FolderTree,
    BulkMoveRequest,
    CommitUploadRequest,
)
from typing import List, Optional, Dict, Any
from datetime import datetime
from pathlib import Path
import uuid as uuid_lib
import json
import aiofiles
# import magic  # Temporarily disabled due to Windows segfault
import logging
from slowapi import Limiter
from slowapi.util import get_remote_address
import asyncio
from io import BytesIO
from functools import lru_cache, wraps
import time

from app.services.chunk_service import chunk_manager

# Path generation utilities for hybrid UUID/name-based system
async def get_filesystem_path(folder_uuid: str, db: AsyncSession) -> str:
    """Build UUID-based path for actual file storage"""
    if not folder_uuid:
        return "/"
    
    # Get the folder
    result = await db.execute(
        select(ArchiveFolder).where(ArchiveFolder.uuid == folder_uuid)
    )
    folder = result.scalar_one_or_none()
    
    if not folder:
        return "/"
    
    # Build path by traversing up the hierarchy using UUIDs
    path_parts = [folder.uuid]
    current_parent = folder.parent_uuid
    
    while current_parent:
        parent_result = await db.execute(
            select(ArchiveFolder).where(ArchiveFolder.uuid == current_parent)
        )
        parent_folder = parent_result.scalar_one_or_none()
        
        if not parent_folder:
            break
            
        path_parts.insert(0, parent_folder.uuid)
        current_parent = parent_folder.parent_uuid
    
    return "/" + "/".join(path_parts) + "/"

async def get_display_path(folder_uuid: str, db: AsyncSession) -> str:
    """Build name-based path for user display"""
    if not folder_uuid:
        return "/"
    
    # Get the folder
    result = await db.execute(
        select(ArchiveFolder).where(ArchiveFolder.uuid == folder_uuid)
    )
    folder = result.scalar_one_or_none()
    
    if not folder:
        return "/"
    
    # Build path by traversing up the hierarchy using names
    path_parts = [folder.name]
    current_parent = folder.parent_uuid
    
    while current_parent:
        parent_result = await db.execute(
            select(ArchiveFolder).where(ArchiveFolder.uuid == current_parent)
        )
        parent_folder = parent_result.scalar_one_or_none()
        
        if not parent_folder:
            break
            
        path_parts.insert(0, parent_folder.name)
        current_parent = parent_folder.parent_uuid
    
    return "/" + "/".join(path_parts) + "/"

async def validate_folder_name(name: str, parent_uuid: Optional[str], user_id: int, db: AsyncSession, exclude_uuid: Optional[str] = None) -> None:
    """Validate folder name is unique at the same level"""
    # Sanitize folder name
    sanitized_name = name.strip()
    if not sanitized_name:
        raise HTTPException(status_code=400, detail="Folder name cannot be empty")
    
    # Check for invalid characters
    invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
    if any(char in sanitized_name for char in invalid_chars):
        raise HTTPException(status_code=400, detail=f"Folder name contains invalid characters: {invalid_chars}")
    
    # Check for duplicate names at same level
    query = select(ArchiveFolder).where(
        and_(
            ArchiveFolder.name == sanitized_name,
            ArchiveFolder.parent_uuid == parent_uuid,
            ArchiveFolder.user_id == user_id
        )
    )
    
    # Exclude current folder when updating
    if exclude_uuid:
        query = query.where(ArchiveFolder.uuid != exclude_uuid)
    
    existing = await db.execute(query)
    if existing.scalar_one_or_none():
        raise HTTPException(status_code=400, detail="Folder name already exists at this level")

# Lightweight imports for file processing with multi-step fallback
try:
    import imagesize  # Lightweight image dimensions (1MB vs 100MB Pillow)
except ImportError:
    imagesize = None

try:
    import filetype  # File type detection with dimensions (2MB)
except ImportError:
    filetype = None

try:
    import pypdf  # Lightweight PDF metadata (5MB vs 50MB PyMuPDF)
except ImportError:
    pypdf = None

try:
    from tinytag import TinyTag  # Lightweight metadata extraction (1MB vs 30MB docx)
except ImportError:
    TinyTag = None

# Legacy heavy packages (fallback if lightweight unavailable)
try:
    import fitz  # PyMuPDF for PDF processing
except ImportError:
    fitz = None

try:
    from docx import Document as DocxDocument  # python-docx for DOCX processing
except ImportError:
    DocxDocument = None

try:
    from PIL import Image  # Pillow (heavy fallback)
except ImportError:
    Image = None

from app.database import get_db
from app.config import settings, get_data_dir, get_file_storage_dir, NEPAL_TZ
from app.models.archive import ArchiveFolder, ArchiveItem
from app.models.tag_associations import archive_tags
from app.models.tag import Tag
from app.models.user import User
from app.auth.dependencies import get_current_user
from app.services.fts_service_enhanced import enhanced_fts_service
# from app.services.ai_service import analyze_content, is_ai_enabled
from app.utils.security import (
    sanitize_folder_name,
    sanitize_filename,
    sanitize_search_query,
    sanitize_description,
    sanitize_tags,
    sanitize_json_metadata,
    validate_file_size,
    validate_uuid_format,
    sanitize_text_input
)
from app.services.file_detection import file_detector

router = APIRouter()

# Initialize rate limiter for file uploads
limiter = Limiter(key_func=get_remote_address)

logger = logging.getLogger(__name__)

# Simple cache decorator for folder operations
def folder_cache(maxsize: int = 128):
    """Simple LRU cache decorator for folder and file listing operations.
    
    Caches folder listings to reduce database load when repeatedly viewing
    the same folders. Cache is per-user and respects query parameters.
    """
    def decorator(func):
        # Create a simple cache key from user_id and critical parameters
        def cache_key(*args, **kwargs):
            # Extract user_id from current_user parameter
            user_id = None
            if 'current_user' in kwargs:
                user_id = getattr(kwargs['current_user'], 'id', None)
            
            # Extract key parameters that affect results
            key_parts = [user_id]
            for param in ['parent_uuid', 'root_uuid', 'archived', 'search']:
                if param in kwargs:
                    key_parts.append((param, kwargs[param]))
            
            return tuple(key_parts)
        
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Generate cache key
            key = cache_key(*args, **kwargs)
            
            # For async functions, we can't use lru_cache directly
            # So we'll use a simple manual cache instead
            if not hasattr(wrapper, '_cache'):
                wrapper._cache = {}
                wrapper._cache_times = {}
            
            cache_timeout = 30  # 30 seconds cache
            now = time.time()
            
            # Check if we have a valid cached result
            if key in wrapper._cache:
                cached_time = wrapper._cache_times.get(key, 0)
                if now - cached_time < cache_timeout:
                    return wrapper._cache[key]
            
            # Cache miss or expired - call the actual function
            result = await func(*args, **kwargs)
            
            # Store in cache
            wrapper._cache[key] = result
            wrapper._cache_times[key] = now
            
            # Simple cache cleanup - remove oldest entries if cache gets too big
            if len(wrapper._cache) > maxsize:
                oldest_key = min(wrapper._cache_times.keys(), 
                               key=lambda k: wrapper._cache_times[k])
                del wrapper._cache[oldest_key]
                del wrapper._cache_times[oldest_key]
            
            return result
        
        # Add cache control methods
        wrapper.cache_clear = lambda: (
            setattr(wrapper, '_cache', {}),
            setattr(wrapper, '_cache_times', {})
        )
        
        return wrapper
    
    return decorator

# Constants
MAX_FOLDER_NAME_LENGTH = 255
MAX_FOLDER_DESCRIPTION_LENGTH = 1000
MAX_FILE_SIZE = settings.max_file_size  # 50MB from config

# Valid MIME types for file uploads
VALID_MIME_TYPES = {
    "application/pdf": ".pdf",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
    "application/msword": ".doc",
    "text/plain": ".txt",
    "text/markdown": ".md",
    "image/jpeg": ".jpg",
    "image/png": ".png",
    "image/gif": ".gif",
    "image/webp": ".webp",
    "audio/mpeg": ".mp3",
    "audio/wav": ".wav",
    "audio/ogg": ".ogg",
    "video/mp4": ".mp4",
    "video/webm": ".webm",
    "application/zip": ".zip",
    "application/x-tar": ".tar",
    "application/gzip": ".gz",
    "application/json": ".json",
    "application/xml": ".xml",
    "text/csv": ".csv",
    "application/vnd.ms-excel": ".xls",
    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx"
}

# Pydantic models are now in app/schemas/archive.py

# Folder Management Endpoints

@router.post("/folders", response_model=FolderResponse)
async def create_folder(
    folder_data: FolderCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Create a new archive folder with enhanced security validation and duplicate prevention"""
    
    try:
        # Check for duplicate folder name in same location
        duplicate_check = await db.execute(
            select(ArchiveFolder).where(
                and_(
                    ArchiveFolder.parent_uuid == folder_data.parent_uuid,
                    func.lower(ArchiveFolder.name) == func.lower(folder_data.name),
                    ArchiveFolder.user_id == current_user.id
                    # Archive folders don't use is_archived flag - all are active by being in archive
                )
            )
        )
        existing_folder = duplicate_check.scalar_one_or_none()
        
        if existing_folder:
            location = "root" if not folder_data.parent_uuid else f"folder '{existing_folder.parent.name}'"
            raise HTTPException(
                status_code=409,
                detail=f"Folder '{folder_data.name}' already exists in {location}. Please choose a different name."
            )
        
        # Validate parent folder exists and belongs to user
        if folder_data.parent_uuid:
            parent_result = await db.execute(
                select(ArchiveFolder).where(
                    and_(
                        ArchiveFolder.uuid == folder_data.parent_uuid,
                        ArchiveFolder.user_id == current_user.id
                    )
                )
            )
            parent = parent_result.scalar_one_or_none()
            if not parent:
                raise HTTPException(
                    status_code=404,
                    detail="Parent folder not found"
                )
        
        # Validate folder name is unique at this level
        await validate_folder_name(folder_data.name, folder_data.parent_uuid, current_user.id, db)
        
        # Create new folder (no path column needed - we generate paths dynamically)
        folder = ArchiveFolder(
            name=folder_data.name.strip(),
            description=folder_data.description,
            parent_uuid=folder_data.parent_uuid,
            user_id=current_user.id
        )
        
        db.add(folder)
        await db.commit()
        await db.refresh(folder)
        
        logger.info(f"âœ… Created folder '{folder.name}' for user {current_user.username}")
        
        # Clear cache after folder creation
        list_folders.cache_clear()
        get_folder_tree.cache_clear()
        
        return await _get_folder_with_stats(db, folder.uuid)
        
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        logger.error(f"âŒ Error creating folder: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create folder. Please try again."
        )

@router.get("/folders", response_model=List[FolderResponse])
@folder_cache()
async def list_folders(
    parent_uuid: Optional[str] = Query(None),
    archived: Optional[bool] = Query(False),
    search: Optional[str] = Query(None),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List folders with FTS5 search on name/description and enhanced security/error handling."""
    from sqlalchemy import text
    try:
        # Validate parent UUID if provided (simplified for single user)
        if parent_uuid:
            parent_uuid = validate_uuid_format(parent_uuid)
            # Verify parent folder exists (no user check needed for single user)
            parent_result = await db.execute(
                select(ArchiveFolder).where(ArchiveFolder.uuid == parent_uuid)
            )
            if not parent_result.scalar_one_or_none():
                raise HTTPException(
                    status_code=404,
                    detail="Parent folder not found"
                )
        # Sanitize search query if provided
        if search:
            search = sanitize_search_query(search)
            # Use centralized FTS5 search
            uuid_list = await enhanced_fts_service.search_archive_folders(db, search, current_user.id)
            if not uuid_list:
                return []
            # Fetch full rows, preserving FTS5 order
            query = select(ArchiveFolder).where(ArchiveFolder.uuid.in_(uuid_list))
            if parent_uuid is not None:
                query = query.where(ArchiveFolder.parent_uuid == parent_uuid)
            # Archive folders don't use is_archived flag - all are active by being in archive
            query = query.order_by(ArchiveFolder.name)
            result = await db.execute(query)
            folders = result.scalars().all()
            folder_map = {f.uuid: f for f in folders}
            folder_responses = []
            for uuid in uuid_list:
                if uuid in folder_map:
                    folder_response = await _get_folder_with_stats(db, uuid)
                    folder_responses.append(folder_response)
            return folder_responses
        else:
            # Build query (simplified for single user)
            query = select(ArchiveFolder)
            query = query.where(ArchiveFolder.parent_uuid == parent_uuid)
            # Archive folders don't use is_archived flag - all are active by being in archive
            query = query.order_by(ArchiveFolder.name)
            result = await db.execute(query)
            folders = result.scalars().all()
            folder_responses = []
            for folder in folders:
                folder_response = await _get_folder_with_stats(db, folder.uuid)
                folder_responses.append(folder_response)
            return folder_responses
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error listing folders: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve folders. Please try again."
        )

@router.get("/folders/tree", response_model=List[FolderTree])
@folder_cache()
async def get_folder_tree(
    root_uuid: Optional[str] = Query(None),
    archived: Optional[bool] = Query(False),
    search: Optional[str] = Query(None),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get hierarchical folder tree structure, or flat FTS5 search results if search is provided."""
    from sqlalchemy import text
    if search:
        # Use centralized FTS5 search for folders
        search = sanitize_search_query(search)
        uuid_list = await enhanced_fts_service.search_archive_folders(db, search, current_user.id)
        if not uuid_list:
            return []
        # Fetch full rows, preserving FTS5 order
        query = select(ArchiveFolder).where(ArchiveFolder.uuid.in_(uuid_list))
        # Archive folders don't use is_archived flag - all are active by being in archive
        result = await db.execute(query)
        folders = result.scalars().all()
        folder_map = {f.uuid: f for f in folders}
        folder_trees = []
        for uuid in uuid_list:
            if uuid in folder_map:
                folder_response = await _get_folder_with_stats(db, uuid)
                folder_trees.append(FolderTree(folder=folder_response, children=[], items=[]))
        return folder_trees
    else:
        async def build_tree(parent_uuid: Optional[str]) -> List[FolderTree]:
            # Get folders
            folder_query = select(ArchiveFolder).where(
                and_(
                    ArchiveFolder.user_id == current_user.id,
                    ArchiveFolder.parent_uuid == parent_uuid
                )
            )
            # Archive folders don't use is_archived flag - all are active by being in archive
            folder_result = await db.execute(folder_query.order_by(ArchiveFolder.name))
            folders = folder_result.scalars().all()
            tree = []
            for folder in folders:
                # Get folder stats
                folder_response = await _get_folder_with_stats(db, folder.uuid)
                # Get items in this folder
                items_query = select(ArchiveItem).where(
                    and_(
                        ArchiveItem.folder_uuid == folder.uuid,
                        ArchiveItem.user_id == current_user.id
                    )
                )
                # Archive items don't need archived filtering - they're all active in archive
                items_result = await db.execute(items_query.order_by(ArchiveItem.name))
                items = items_result.scalars().all()
                item_summaries = []
                for item in items:
                    item_summary = await _get_item_summary(db, item.uuid)
                    item_summaries.append(item_summary)
                # Recursively build children
                children = await build_tree(folder.uuid)
                tree.append(FolderTree(
                    folder=folder_response,
                    children=children,
                    items=item_summaries
                ))
            return tree
        return await build_tree(root_uuid)

@router.get("/folders/{folder_uuid}/breadcrumb", response_model=List[FolderResponse])
async def get_folder_breadcrumb(
    folder_uuid: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get the breadcrumb trail for a specific folder."""
    breadcrumb = []
    current_folder_uuid = folder_uuid

    while current_folder_uuid:
        folder_query = select(ArchiveFolder).where(
            ArchiveFolder.uuid == current_folder_uuid,
            ArchiveFolder.user_id == current_user.id
        )
        result = await db.execute(folder_query)
        folder = result.scalar_one_or_none()

        if not folder:
            # This can happen if a parent in the chain is not found
            # or doesn't belong to the user. Stop here.
            break
        
        # This uses an internal helper, but it's efficient
        folder_response = await _get_folder_with_stats(db, folder.uuid)
        breadcrumb.append(folder_response)
        
        current_folder_uuid = folder.parent_uuid

    # The breadcrumb is built from child to parent, so reverse it
    return breadcrumb[::-1]

@router.get("/folders/{folder_uuid}", response_model=FolderResponse)
async def get_folder(
    folder_uuid: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get folder details (simplified for single user)"""
    folder_uuid = validate_uuid_format(folder_uuid)
    return await _get_folder_with_stats(db, folder_uuid)

@router.put("/folders/{folder_uuid}", response_model=FolderResponse)
async def update_folder(
    folder_uuid: str,
    folder_data: FolderUpdate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Update folder (simplified for single user)"""
    
    try:
        folder_uuid = validate_uuid_format(folder_uuid)
        
        # Get folder (simplified for single user)
        result = await db.execute(
            select(ArchiveFolder).where(ArchiveFolder.uuid == folder_uuid)
        )
        folder = result.scalar_one_or_none()
        
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # Update folder fields
        if folder_data.name is not None:
            folder.name = folder_data.name
        if folder_data.description is not None:
            folder.description = folder_data.description
        
        await db.commit()
        await db.refresh(folder)
        
        # Clear cache after folder update
        list_folders.cache_clear()
        get_folder_tree.cache_clear()
        
        return await _get_folder_with_stats(db, folder.uuid)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error updating folder: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to update folder. Please try again."
        )

@router.delete("/folders/{folder_uuid}")
async def delete_folder(
    folder_uuid: str,
    force: bool = Query(False, description="Force delete non-empty folder"),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Delete folder (simplified for single user)"""
    
    try:
        folder_uuid = validate_uuid_format(folder_uuid)
        
        # Get folder (simplified for single user)
        result = await db.execute(
            select(ArchiveFolder).where(ArchiveFolder.uuid == folder_uuid)
        )
        folder = result.scalar_one_or_none()
        
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # Check if folder has contents
        if not force:
            # Check for subfolders
            subfolder_result = await db.execute(
                select(func.count(ArchiveFolder.uuid)).where(
                    ArchiveFolder.parent_uuid == folder_uuid
                )
            )
            subfolder_count = subfolder_result.scalar()
            
            # Check for items
            item_result = await db.execute(
                select(func.count(ArchiveItem.uuid)).where(
                    ArchiveItem.folder_uuid == folder_uuid
                )
            )
            item_count = item_result.scalar()
            
            if subfolder_count > 0 or item_count > 0:
                raise HTTPException(
                    status_code=400,
                    detail="Folder is not empty. Use force=true to delete non-empty folder."
                )
        
        # Delete folder and all contents (CASCADE handles this atomically)
        await db.delete(folder)
        await db.commit()
        
        logger.info(f"âœ… Deleted folder '{folder.name}' for user {current_user.username}")
        
        # Clear cache after folder deletion
        list_folders.cache_clear()
        get_folder_tree.cache_clear()
        
        return {"message": "Folder deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error deleting folder: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete folder. Please try again."
        )

# ---------------------------
# Bulk operations
# ---------------------------



@router.post("/bulk/move")
async def bulk_move_items(
    payload: BulkMoveRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Move many items to a different folder in one transaction.

    Accepts a JSON body:
    {
      "items": ["item-uuid-1", "item-uuid-2"],
      "target_folder": "folder-uuid"
    }
    """

    # Validate destination folder exists
    tgt = await db.execute(
        select(ArchiveFolder).where(and_(ArchiveFolder.uuid == payload.target_folder))
    )
    target_folder_obj = tgt.scalar_one_or_none()
    if not target_folder_obj:
        raise HTTPException(status_code=404, detail="Target folder not found")

    # Fetch all items to move â€“ ensure they belong to user
    result = await db.execute(
        select(ArchiveItem).where(ArchiveItem.uuid.in_(payload.items))
    )
    items_to_move = result.scalars().all()

    if len(items_to_move) != len(payload.items):
        missing = set(payload.items) - {it.uuid for it in items_to_move}
        raise HTTPException(status_code=404, detail={"message": "Some items not found", "missing": list(missing)})

    # Update folder_uuid for each item
    for item in items_to_move:
        item.folder_uuid = payload.target_folder

    await db.commit()

    return {"moved": len(items_to_move), "target_folder": payload.target_folder}

# Item Management Endpoints

@router.post("/folders/{folder_uuid}/items", response_model=ItemResponse)
@limiter.limit("10/minute")  # Rate limit file uploads to 10 per minute
async def upload_item(
    folder_uuid: str,
    request: Request,  # Required for rate limiting - moved before optional params
    file: UploadFile = File(...),
    name: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),  # JSON string of tag list
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Upload a new item to a folder with enhanced security and validation"""
    
    try:
        # Validate folder UUID format
        folder_uuid = validate_uuid_format(folder_uuid)
        
        # Verify folder exists (simplified for single user)
        result = await db.execute(
            select(ArchiveFolder).where(ArchiveFolder.uuid == folder_uuid)
        )
        folder = result.scalar_one_or_none()
        if not folder:
            raise HTTPException(status_code=404, detail="Folder not found")
        
        # Validate file is provided
        if not file or not file.filename:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No file provided"
            )
        
        # Read file content and validate size first
        file_content = await file.read()
        file_size = len(file_content)
        
        # Validate file size using security function
        validate_file_size(file_size, MAX_FILE_SIZE)
        
        # Reset file pointer for further processing
        await file.seek(0)
        
        # Detect MIME type from content (more secure than trusting filename)
        # mime_type = magic.from_buffer(file_content[:2048], mime=True)
        # Temporarily use file extension-based detection due to magic segfault
        import mimetypes
        mime_type, _ = mimetypes.guess_type(file.filename)
        if not mime_type:
            mime_type = "application/octet-stream"
        
        # Validate MIME type
        if mime_type not in VALID_MIME_TYPES:
            raise HTTPException(
                status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,
                detail=f"Unsupported file type: {mime_type}. Supported types: {', '.join(VALID_MIME_TYPES.values())}"
            )
        
        # Sanitize and validate inputs
        original_filename = sanitize_filename(file.filename)
        item_name = sanitize_filename(name) if name else Path(original_filename).stem
        item_description = sanitize_description(description) if description else None
        
        # Check for duplicate file name in same folder
        duplicate_check = await db.execute(
            select(ArchiveItem).where(
                and_(
                    ArchiveItem.folder_uuid == folder_uuid,
                    func.lower(ArchiveItem.name) == func.lower(item_name),
                    ArchiveItem.user_id == current_user.id
                    # Archive items don't need archived filtering - they're all active in archive
                )
            )
        )
        existing_file = duplicate_check.scalar_one_or_none()
        
        if existing_file:
            raise HTTPException(
                status_code=409,
                detail=f"File '{item_name}' already exists in this folder. Please rename the file or choose a different folder."
            )
        
        # Parse and sanitize tags
        tag_list = []
        if tags:
            try:
                parsed_tags = json.loads(tags)
                if isinstance(parsed_tags, list):
                    tag_list = sanitize_tags(parsed_tags)
            except json.JSONDecodeError:
                # If JSON parsing fails, treat as single tag
                tag_list = sanitize_tags([tags])
        
        # Generate safe filename for storage
        file_extension = VALID_MIME_TYPES[mime_type]
        stored_filename = f"{uuid_lib.uuid4()}{file_extension}"
        
        # Create storage directory using UUID-based path hierarchy
        filesystem_path = await get_filesystem_path(folder_uuid, db)
        storage_dir = Path(get_file_storage_dir()) / "archive" / filesystem_path.strip("/")
        storage_dir.mkdir(parents=True, exist_ok=True)
        
        # Save file to TEMPORARY location first
        temp_file_path = storage_dir / f"temp_{stored_filename}"
        final_file_path = storage_dir / stored_filename
        try:
            async with aiofiles.open(temp_file_path, 'wb') as f:
                await f.write(file_content)
        except Exception as e:
            logger.error(f"âŒ Failed to save file: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to save file to storage"
            )
        
        # AI analysis for smart tagging (with error handling)
        ai_tags = []
        # TODO: Re-enable AI analysis when available
        # if is_ai_enabled() and extracted_text:
        #     try:
        #         analysis = await analyze_content(extracted_text, "archive")
        #         ai_tags = analysis.get("tags", [])
        #         # Sanitize AI-generated tags
        #         ai_tags = sanitize_tags(ai_tags)
        #     except Exception as e:
        #         logger.warning(f"âš ï¸ AI analysis failed: {str(e)}")
        #         # Continue without AI tags
        
        # Combine tags (user tags take precedence)
        all_tags = list(set(tag_list + ai_tags))
        
        # Create item using shared helper
        item = await _create_archive_item(
            db=db,
            file_path=final_file_path,  # Use final path for DB record
            folder_uuid=folder_uuid,
            original_filename=original_filename,
            stored_filename=stored_filename,
            mime_type=mime_type,
            file_size=file_size,
            user_id=current_user.id,
            name=item_name,
            description=item_description,
            tags=all_tags
        )
        
        await db.commit()
        await db.refresh(item)
        
        # SECURITY: Move file to final location ONLY after successful DB commit
        try:
            temp_file_path.rename(final_file_path)
            logger.info(f"âœ… File moved to final location: {final_file_path}")
        except Exception as move_error:
            logger.error(f"âŒ Failed to move file to final location: {move_error}")
            # Clean up temp file
            try:
                if temp_file_path.exists():
                    temp_file_path.unlink()
            except Exception:
                pass
            raise HTTPException(status_code=500, detail="Failed to finalize file storage")
        
        logger.info(f"âœ… Uploaded file '{original_filename}' to folder '{folder.name}' for user {current_user.username}")
        
        return await _get_item_with_relations(db, item.uuid)
        
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        # Clean up temp file on HTTP exception
        try:
            if 'temp_file_path' in locals() and temp_file_path.exists():
                temp_file_path.unlink()
        except Exception:
            pass
        raise
    except Exception as e:
        logger.error(f"âŒ Error uploading file: {str(e)}")
        await db.rollback()
        # Clean up temp file on DB rollback
        try:
            if 'temp_file_path' in locals() and temp_file_path.exists():
                temp_file_path.unlink()
        except Exception:
            pass
        
        # Clean up partially uploaded file if it exists (legacy cleanup)
        try:
            if 'final_file_path' in locals() and final_file_path.exists():
                final_file_path.unlink()
        except Exception as cleanup_error:
            logger.error(f"âŒ Failed to cleanup file: {str(cleanup_error)}")
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to upload file. Please try again."
        )

@router.get("/folders/{folder_uuid}/items", response_model=List[ItemSummary])
async def list_folder_items(
    folder_uuid: str,
    archived: Optional[bool] = Query(False),
    search: Optional[str] = Query(None),
    mime_type: Optional[str] = Query(None),
    tag: Optional[str] = Query(None),
    limit: int = Query(50, le=100),
    offset: int = Query(0, ge=0),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """List items in folder"""
    
    # Verify folder access
    folder_result = await db.execute(
        select(ArchiveFolder).where(
            and_(
                ArchiveFolder.uuid == folder_uuid,
                ArchiveFolder.user_id == current_user.id
            )
        )
    )
    if not folder_result.scalar_one_or_none():
        raise HTTPException(status_code=404, detail="Folder not found")
    
    # Build query
    query = select(ArchiveItem).where(
        and_(
            ArchiveItem.folder_uuid == folder_uuid,
            ArchiveItem.user_id == current_user.id
        )
    )
    
    # Filters
    # Archive items don't need archived filtering - they're all active in archive
    
    if search:
        search_term = f"%{search}%"
        query = query.where(
            or_(
                ArchiveItem.name.ilike(search_term),
                ArchiveItem.original_filename.ilike(search_term)
            )
        )
    
    if mime_type:
        if mime_type.endswith('/'):
            query = query.where(ArchiveItem.mime_type.like(f"{mime_type}%"))
        else:
            query = query.where(ArchiveItem.mime_type == mime_type)
    
    if tag:
        # Join with tags to filter by tag
        query = query.join(archive_tags).join(Tag).where(Tag.name == tag)
    
    # Pagination and ordering
    query = query.order_by(desc(ArchiveItem.updated_at)).offset(offset).limit(limit)
    
    result = await db.execute(query)
    items = result.scalars().all()
    
    # Convert to summaries
    summaries = []
    for item in items:
        summary = await _get_item_summary(db, item.uuid)
        summaries.append(summary)
    
    return summaries

@router.get("/items/{item_uuid}", response_model=ItemResponse)
async def get_item(
    item_uuid: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get item details (simplified for single user)"""
    item_uuid = validate_uuid_format(item_uuid)
    return await _get_item_with_relations(db, item_uuid)

@router.put("/items/{item_uuid}", response_model=ItemResponse)
async def update_item(
    item_uuid: str,
    item_data: ItemUpdate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Update item (simplified for single user)"""
    
    try:
        item_uuid = validate_uuid_format(item_uuid)
        
        # Get item (simplified for single user)
        result = await db.execute(
            select(ArchiveItem).where(ArchiveItem.uuid == item_uuid)
        )
        item = result.scalar_one_or_none()
        
        if not item:
            raise HTTPException(status_code=404, detail="Item not found")
        
        # Validate folder if changing
        if item_data.folder_uuid and item_data.folder_uuid != item.folder_uuid:
            folder_result = await db.execute(
                select(ArchiveFolder).where(ArchiveFolder.uuid == item_data.folder_uuid)
            )
            if not folder_result.scalar_one_or_none():
                raise HTTPException(status_code=404, detail="Target folder not found")
        
        # Update item fields
        if item_data.name is not None:
            item.name = item_data.name
        if item_data.description is not None:
            item.description = item_data.description
        if item_data.folder_uuid is not None:
            item.folder_uuid = item_data.folder_uuid
        if item_data.is_favorite is not None:
            item.is_favorite = item_data.is_favorite
        
        # Handle tags
        if item_data.tags is not None:
            await _handle_item_tags(db, item, item_data.tags)
        
        await db.commit()
        await db.refresh(item)
        
        return await _get_item_with_relations(db, item.uuid)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error updating item: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to update item. Please try again."
        )

@router.delete("/items/{item_uuid}")
async def delete_item(
    item_uuid: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Delete item (simplified for single user)"""
    
    try:
        item_uuid = validate_uuid_format(item_uuid)
        
        # Get item (simplified for single user)
        result = await db.execute(
            select(ArchiveItem).where(ArchiveItem.uuid == item_uuid)
        )
        item = result.scalar_one_or_none()
        
        if not item:
            raise HTTPException(status_code=404, detail="Item not found")
        
        # Delete file from storage
        if item.file_path and Path(item.file_path).exists():
            try:
                Path(item.file_path).unlink()
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to delete file: {str(e)}")
        
        # Delete thumbnail if exists
        if item.thumbnail_path and Path(item.thumbnail_path).exists():
            try:
                Path(item.thumbnail_path).unlink()
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to delete thumbnail: {str(e)}")
        
        # Delete item from database
        await db.delete(item)
        await db.commit()
        
        logger.info(f"âœ… Deleted item '{item.name}' for user {current_user.username}")
        
        return {"message": "Item deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error deleting item: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete item. Please try again."
        )

@router.get("/items/{item_uuid}/download")
async def download_item(
    item_uuid: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Download item file"""
    
    # Get item
    result = await db.execute(
        select(ArchiveItem).where(
            and_(
                ArchiveItem.uuid == item_uuid,
                ArchiveItem.user_id == current_user.id
            )
        )
    )
    item = result.scalar_one_or_none()
    if not item:
        raise HTTPException(status_code=404, detail="Item not found")
    
    file_path = Path(item.file_path)
    
    # SECURITY: Resolve symlinks and validate path is within allowed directory
    try:
        resolved_path = file_path.resolve()
        # Ensure the resolved path is within the archive directory
        archive_base = Path(get_file_storage_dir()) / "archive"
        archive_base = archive_base.resolve()
        
        # Windows-safe path validation - avoid relative_to() which fails on different drives
        try:
            # Try relative_to first (works on same drive)
            relative_path = resolved_path.relative_to(archive_base)
            # SECURITY: Comprehensive path traversal check
            relative_str = str(relative_path)
            # Check for various path traversal patterns
            if (".." in relative_str or 
                "../" in relative_str or 
                "..\\" in relative_str or
                relative_str.startswith("..") or
                "\\.." in relative_str or
                "/.." in relative_str):
                raise HTTPException(status_code=403, detail="Access denied: Path traversal detected")
        except ValueError:
            # relative_to failed - likely different drives on Windows
            # Use string comparison as fallback
            resolved_str = str(resolved_path)
            archive_str = str(archive_base)
            if not resolved_str.startswith(archive_str):
                raise HTTPException(status_code=403, detail="Access denied: Invalid file path")
    except (OSError, ValueError) as e:
        raise HTTPException(status_code=400, detail=f"Invalid file path: {str(e)}")
    
    if not resolved_path.exists():
        raise HTTPException(status_code=404, detail="File not found on disk")
    
    return FileResponse(
        path=str(resolved_path),
        filename=item.original_filename,
        media_type=item.mime_type
    )

# Search Endpoints

@router.get("/search", response_model=List[ItemResponse])
async def search_items(
    query: str,
    folder_uuid: Optional[str] = None,
    mime_type: Optional[str] = None,
    tag: Optional[str] = None,
    include_archived: bool = False,
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Search for items with FTS5 (name, filename, description, metadata, tags) and filter by tag/mime type."""
    from sqlalchemy import text
    try:
        # Sanitize search query and use centralized FTS5 search
        search_query = sanitize_search_query(query)
        uuid_list = await enhanced_fts_service.search_archive_items(
            db, search_query, current_user.id, tag=tag, 
            limit=page_size, offset=(page-1)*page_size
        )
        if not uuid_list:
            return []
        # Fetch full rows, preserving FTS5 order
        item_query = (
            select(ArchiveItem)
            .options(selectinload(ArchiveItem.tag_objs))
            .where(ArchiveItem.uuid.in_(uuid_list))
        )
        # Archive items don't need archived filtering - they're all active in archive
        if folder_uuid:
            item_query = item_query.where(ArchiveItem.folder_uuid == folder_uuid)
        # Filter by mime_type after FTS5
        if mime_type:
            if mime_type.endswith('/'):
                item_query = item_query.where(ArchiveItem.mime_type.like(f"{mime_type}%"))
            else:
                item_query = item_query.where(ArchiveItem.mime_type == mime_type)
        item_result = await db.execute(item_query)
        items = item_result.scalars().all()
        # Map uuid to item for FTS5 order
        item_map = {item.uuid: item for item in items}
        item_responses = []
        for uuid in uuid_list:
            if uuid in item_map:
                item_response = await _get_item_with_relations(db, uuid)
                item_responses.append(item_response)
        logger.info(f"ðŸ” FTS5 Search completed: '{query}' - {len(item_responses)} results for user {current_user.username}")
        return item_responses
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ FTS5 search error: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Search failed. Please try again."
        )

# Helper functions

async def _get_folder_with_stats(db: AsyncSession, folder_uuid: str) -> FolderResponse:
    """Get folder with statistics (simplified for single user)"""
    
    # Get folder
    result = await db.execute(
        select(ArchiveFolder).where(ArchiveFolder.uuid == folder_uuid)
    )
    folder = result.scalar_one_or_none()
    
    if not folder:
        raise HTTPException(status_code=404, detail="Folder not found")
    
    # Get item count
    item_count_result = await db.execute(
        select(func.count(ArchiveItem.uuid)).where(
            ArchiveItem.folder_uuid == folder_uuid
            # Archive items don't use is_archived flag - all are active by being in archive
        )
    )
    item_count = item_count_result.scalar() or 0
    
    # Get subfolder count
    subfolder_count_result = await db.execute(
        select(func.count(ArchiveFolder.uuid)).where(
            ArchiveFolder.parent_uuid == folder_uuid
            # Archive folders don't use is_archived flag - all are active by being in archive
        )
    )
    subfolder_count = subfolder_count_result.scalar() or 0
    
    # Get total size
    size_result = await db.execute(
        select(func.sum(ArchiveItem.file_size)).where(
            ArchiveItem.folder_uuid == folder_uuid
            # Archive items don't use is_archived flag - all are active by being in archive
        )
    )
    total_size = size_result.scalar() or 0
    
    # Generate both path types dynamically
    display_path = await get_display_path(folder.uuid, db)
    filesystem_path = await get_filesystem_path(folder.uuid, db)
    
    return FolderResponse(
        uuid=folder.uuid,
        name=folder.name,
        description=folder.description,
        parent_uuid=folder.parent_uuid,
        path=display_path,  # Use display path for backward compatibility
        display_path=display_path,  # Human-readable path
        filesystem_path=filesystem_path,  # UUID-based path for debugging
        created_at=folder.created_at,
        updated_at=folder.updated_at,
        item_count=item_count,
        subfolder_count=subfolder_count,
        total_size=total_size
    )


async def _get_item_with_relations(db: AsyncSession, item_uuid: str) -> ItemResponse:
    """Get item with all relationships (simplified for single user)"""
    
    # Get item with tags
    result = await db.execute(
        select(ArchiveItem)
        .options(selectinload(ArchiveItem.tag_objs))
        .where(ArchiveItem.uuid == item_uuid)
    )
    item = result.scalar_one_or_none()
    
    if not item:
        raise HTTPException(status_code=404, detail="Item not found")
    
    # Parse metadata
    metadata = {}
    if item.metadata_json:
        try:
            metadata = json.loads(item.metadata_json)
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON metadata for item {item_uuid}")
    
    # Get tag names
    tag_names = [tag.name for tag in item.tag_objs]
    
    return ItemResponse(
        uuid=item.uuid,
        name=item.name,
        description=item.description,
        folder_uuid=item.folder_uuid,
        original_filename=item.original_filename,
        stored_filename=item.stored_filename,
        mime_type=item.mime_type,
        file_size=item.file_size,
        metadata=metadata,
        thumbnail_path=item.thumbnail_path,
        is_favorite=item.is_favorite,
        version=item.version,
        created_at=item.created_at,
        updated_at=item.updated_at,
        tags=tag_names
    )


async def _get_item_summary(db: AsyncSession, item_uuid: str) -> ItemSummary:
    """Get item summary (simplified for single user)"""
    
    # Get item with tags
    result = await db.execute(
        select(ArchiveItem)
        .options(selectinload(ArchiveItem.tag_objs))
        .where(ArchiveItem.uuid == item_uuid)
    )
    item = result.scalar_one_or_none()
    
    if not item:
        raise HTTPException(status_code=404, detail="Item not found")
    
    # Get tag names
    tag_names = [tag.name for tag in item.tag_objs]
    
    # Create preview (placeholder for now)
    preview = ""
    
    return ItemSummary(
        uuid=item.uuid,
        name=item.name,
        folder_uuid=item.folder_uuid,
        original_filename=item.original_filename,
        mime_type=item.mime_type,
        file_size=item.file_size,
        is_favorite=item.is_favorite,
        created_at=item.created_at,
        updated_at=item.updated_at,
        tags=tag_names,
        preview=preview
    )

async def _handle_item_tags(db: AsyncSession, item: ArchiveItem, tag_names: List[str]):
    """Handle item tag associations with proper module_type"""
    
    # Fetch existing associations
    existing_rows = await db.execute(
        select(archive_tags.c.tag_uuid).where(archive_tags.c.item_uuid == item.uuid)
    )
    existing_tag_uuids = {row[0] for row in existing_rows.fetchall()}

    # Clear existing tags
    await db.execute(delete(archive_tags).where(archive_tags.c.item_uuid == item.uuid))

    removed_tag_uuids = existing_tag_uuids.copy()
    normalized_names = [t.strip() for t in (tag_names or []) if t and t.strip()]
    if not normalized_names:
        # Decrement counts for all removed tags
        if removed_tag_uuids:
            tag_rows = await db.execute(select(Tag).where(Tag.uuid.in_(removed_tag_uuids)))
            for tag in tag_rows.scalars():
                if tag.usage_count > 0:
                    tag.usage_count -= 1
        return

    for tag_name in normalized_names:
        # Get or create tag with proper module_type
        result = await db.execute(
            select(Tag).where(and_(
                Tag.name == tag_name,
                Tag.user_id == item.user_id,
                Tag.module_type == "archive"
            ))
        )
        tag = result.scalar_one_or_none()
        
        if not tag:
            # Create new tag with archive module_type
            tag = Tag(
                name=tag_name, 
                user_id=item.user_id,
                module_type="archive",
                usage_count=1,
                color="#8b5cf6"  # Purple color for archive tags
            )
            db.add(tag)
            await db.flush()
        else:
            # Adjust removed set and increment usage count
            if tag.uuid in removed_tag_uuids:
                removed_tag_uuids.remove(tag.uuid)
            tag.usage_count += 1
        
        # Create association
        await db.execute(
            archive_tags.insert().values(
                item_uuid=item.uuid,
                tag_uuid=tag.uuid
            )
        )

    # Decrement counts for tags that were removed
    if removed_tag_uuids:
        tag_rows = await db.execute(select(Tag).where(Tag.uuid.in_(removed_tag_uuids)))
        for tag in tag_rows.scalars():
            if tag.usage_count > 0:
                tag.usage_count -= 1

# --- START: COPIED FROM documents.py router ---

# Text extraction functionality removed as requested

async def _extract_metadata(file_path: Path, mime_type: str, original_name: str) -> Dict[str, Any]:
    """Extract metadata from file with multi-step lightweight fallback"""
    # Basic metadata
    metadata = {
        "original_name": original_name,
        "mime_type": mime_type,
        "size": file_path.stat().st_size
    }

    # IMAGE METADATA - Multi-step fallback
    if mime_type.startswith('image/'):
        # Step 1: Try imagesize (1MB - ultra lightweight)
        if imagesize:
            try:
                width, height = imagesize.get(file_path)
                if width and height:
                    metadata.update({
                        "width": width,
                        "height": height,
                        "extraction_method": "imagesize (lightweight)"
                    })
            except Exception as e:
                logger.debug(f"imagesize extraction failed: {e}")

        # Step 2: Try filetype (2MB - format detection only)
        if filetype and "format" not in metadata:
            try:
                kind = filetype.guess(file_path)
                if kind and kind.extension:
                    metadata.update({
                        "format": kind.extension.replace('.', '').upper(),
                        "extraction_method": "filetype (format detection)"
                    })
            except Exception as e:
                logger.debug(f"filetype extraction failed: {e}")

        # Step 3: Try Pillow (100MB - heavy fallback)
        if Image and ("width" not in metadata or "height" not in metadata):
            try:
                with Image.open(file_path) as img:
                    metadata.update({
                        "width": img.width,
                        "height": img.height,
                        "format": img.format,
                        "mode": img.mode,
                        "extraction_method": "PIL (heavy fallback)"
                    })
            except ImportError:
                pass  # PIL not installed
            except Exception as e:
                logger.debug(f"PIL extraction failed: {e}")

    # PDF METADATA - Multi-step fallback
    elif mime_type == 'application/pdf':
        # Step 1: Try pypdf (5MB - lightweight)
        if pypdf:
            try:
                with open(file_path, 'rb') as f:
                    reader = pypdf.PdfReader(f)
                    metadata.update({
                        "page_count": len(reader.pages),
                        "pdf_metadata": dict(reader.metadata or {}),
                        "extraction_method": "pypdf (lightweight)"
                    })
            except Exception as e:
                logger.debug(f"pypdf extraction failed: {e}")

        # Step 2: Try PyMuPDF (50MB - heavy fallback)
        if fitz and "page_count" not in metadata:
            try:
                with fitz.open(file_path) as doc:
                    metadata.update({
                        "page_count": doc.page_count,
                        "pdf_metadata": doc.metadata,
                        "extraction_method": "PyMuPDF (heavy fallback)"
                    })
            except Exception as e:
                logger.debug(f"PyMuPDF extraction failed: {e}")

    # DOCUMENT METADATA - Multi-step fallback
    elif mime_type in ['application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'application/msword']:
        # Step 1: Try TinyTag (1MB - ultra lightweight)
        if TinyTag:
            try:
                tag = TinyTag.get(file_path)
                if tag:
                    metadata.update({
                        "title": tag.title,
                        "author": tag.artist,  # TinyTag uses artist for author
                        "duration": tag.duration,
                        "extraction_method": "TinyTag (lightweight)"
                    })
            except Exception as e:
                logger.debug(f"TinyTag extraction failed: {e}")

        # Step 2: Try python-docx (30MB - heavy fallback)
        if DocxDocument and "title" not in metadata:
            try:
                doc = DocxDocument(file_path)
                core_props = doc.core_properties
                metadata.update({
                    "title": core_props.title or "",
                    "author": core_props.author or "",
                    "created": str(core_props.created) if core_props.created else "",
                    "modified": str(core_props.modified) if core_props.modified else "",
                    "extraction_method": "python-docx (heavy fallback)"
                })
            except Exception as e:
                logger.debug(f"python-docx extraction failed: {e}")

    # AUDIO METADATA - Use TinyTag if available
    elif mime_type.startswith('audio/'):
        if TinyTag:
            try:
                tag = TinyTag.get(file_path)
                if tag:
                    metadata.update({
                        "title": tag.title,
                        "artist": tag.artist,
                        "album": tag.album,
                        "duration": tag.duration,
                        "year": tag.year,
                        "extraction_method": "TinyTag (lightweight)"
                    })
            except Exception as e:
                logger.debug(f"TinyTag audio extraction failed: {e}")

    return metadata

# --- END: COPIED FROM documents.py router ---

async def _create_archive_item(
    db: AsyncSession,
    file_path: Path,
    folder_uuid: str,
    original_filename: str,
    stored_filename: str,
    mime_type: str,
    file_size: int,
    user_id: int,
    name: Optional[str] = None,
    description: Optional[str] = None,
    tags: Optional[List[str]] = None,
    additional_metadata: Optional[Dict[str, Any]] = None
) -> ArchiveItem:
    """
    Shared helper function to create archive items with consistent logic
    
    This centralizes the file processing, text extraction, metadata generation,
    and tag handling to ensure consistency across all upload endpoints.
    """
    
    try:
        # Generate metadata
        metadata = {}
        try:
            metadata = await _extract_metadata(file_path, mime_type, original_filename)
            # Add any additional metadata
            if additional_metadata:
                metadata.update(additional_metadata)
            metadata = sanitize_json_metadata(metadata)
        except Exception as e:
            logger.warning(f"âš ï¸ Metadata extraction failed: {str(e)}")
            metadata = additional_metadata or {}
        
        # Create item name
        item_name = sanitize_filename(name) if name else Path(original_filename).stem
        item_description = sanitize_description(description) if description else None
        
        # Create archive item
        item = ArchiveItem(
            name=item_name,
            description=item_description,
            folder_uuid=folder_uuid,
            original_filename=original_filename,
            stored_filename=stored_filename,
            file_path=str(file_path),
            mime_type=mime_type,
            file_size=file_size,
            metadata_json=json.dumps(metadata),
            user_id=user_id
        )
        
        db.add(item)
        await db.flush()  # Get the UUID for tag handling
        
        # Handle tags if provided
        if tags:
            try:
                await _handle_item_tags(db, item, tags)
            except Exception as e:
                logger.warning(f"âš ï¸ Tag handling failed: {str(e)}")
        
        return item
        
    except Exception as e:
        logger.error(f"âŒ Error creating archive item: {str(e)}")
        raise

@router.post("/upload")
async def upload_files(
    files: List[UploadFile] = File(...),
    folder_uuid: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Upload multiple files to the archive"""
    
    logger.info(f"ðŸ“ Starting upload of {len(files)} files to folder {folder_uuid}")
    
    try:
        # Verify folder exists if specified
        if folder_uuid:
            folder_query = select(ArchiveFolder).where(
                and_(
                    ArchiveFolder.uuid == folder_uuid,
                    ArchiveFolder.user_id == current_user.id
                )
            )
            folder_result = await db.execute(folder_query)
            folder = folder_result.scalar_one_or_none()
            if not folder:
                raise HTTPException(status_code=404, detail="Folder not found")
        
        # Parse tags
        tag_list = []
        if tags:
            tag_names = [tag.strip() for tag in tags.split(",") if tag.strip()]
            for tag_name in tag_names:
                # Find or create tag
                tag_query = select(Tag).where(
                    and_(
                        Tag.name == tag_name,
                        Tag.module_type == "archive",
                        Tag.user_id == current_user.id
                    )
                )
                tag_result = await db.execute(tag_query)
                tag = tag_result.scalar_one_or_none()
                if not tag:
                    tag = Tag(
                        name=tag_name,
                        module_type="archive",
                        user_id=current_user.id,
                        color="#6366f1"
                    )
                    db.add(tag)
                    await db.flush()
                tag_list.append(tag)
        
        uploaded_files = []
        temp_file_paths = []  # Track temp paths for cleanup
        
        for file in files:
            try:
                # Validate file input
                if not file or not file.filename:
                    logger.warning(f"âš ï¸ Skipping invalid file in bulk upload")
                    continue
                    
                # Validate file size
                file_content = await file.read()
                file_size = len(file_content)
                
                try:
                    validate_file_size(file_size, MAX_FILE_SIZE)
                except Exception as e:
                    logger.warning(f"âš ï¸ Skipping file '{file.filename}' due to size validation: {str(e)}")
                    continue
                
                # Generate unique filename
                file_uuid = str(uuid_lib.uuid4())
                file_extension = Path(file.filename).suffix
                safe_filename = f"{file_uuid}{file_extension}"
                
                # Create storage directory using UUID-based path hierarchy
                if folder_uuid:
                    filesystem_path = await get_filesystem_path(folder_uuid, db)
                    storage_dir = Path(get_file_storage_dir()) / "archive" / filesystem_path.strip("/")
                else:
                    # If no folder specified, use a default "unorganized" folder
                    storage_dir = Path(get_file_storage_dir()) / "archive" / "unorganized"
                storage_dir.mkdir(parents=True, exist_ok=True)
                
                temp_file_path = storage_dir / f"temp_{safe_filename}"
                final_file_path = storage_dir / safe_filename
                
                # Detect file type using our enhanced service
                try:
                    detection_result = await file_detector.detect_file_type(
                        file_path=None,  # Use file_content instead of client path
                        file_content=file_content
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ File detection failed for '{file.filename}': {str(e)}")
                    # Use fallback detection
                    import mimetypes
                    mime_type, _ = mimetypes.guess_type(file.filename)
                    detection_result = {
                        "mime_type": mime_type or "application/octet-stream",
                        "confidence": "low",
                        "detection_method": "fallback"
                    }
                
                # Validate MIME type
                if detection_result["mime_type"] not in VALID_MIME_TYPES:
                    logger.warning(f"âš ï¸ Skipping file '{file.filename}' due to unsupported MIME type: {detection_result['mime_type']}")
                    continue
                
                # Save file to TEMPORARY location with error handling
                try:
                    async with aiofiles.open(temp_file_path, 'wb') as f:
                        await f.write(file_content)
                    # Track temp path for later cleanup
                    temp_file_paths.append((temp_file_path, final_file_path))
                except Exception as e:
                    logger.error(f"âŒ Failed to save file '{file.filename}': {str(e)}")
                    continue
                
                # Create archive item using shared helper
                additional_metadata = {
                    "upload_info": {
                        "original_name": file.filename,
                        "upload_date": datetime.now(NEPAL_TZ).isoformat(),
                        "file_detection": detection_result
                    }
                }
                
                # Extract tag names for the shared helper
                tag_names = [tag.name for tag in tag_list] if tag_list else None
                
                archive_item = await _create_archive_item(
                    db=db,
                    file_path=final_file_path,  # Use final path for DB record
                    folder_uuid=folder_uuid,
                    original_filename=file.filename,
                    stored_filename=safe_filename,
                    mime_type=detection_result["mime_type"],
                    file_size=len(file_content),
                    user_id=current_user.id,
                    name=Path(file.filename).stem,
                    description=description,
                    tags=tag_names,
                    additional_metadata=additional_metadata
                )
                
                # Note: archive_item.uuid is set by the shared helper
                
                uploaded_files.append({
                    "uuid": archive_item.uuid,
                    "name": archive_item.name,
                    "filename": archive_item.original_filename,
                    "mime_type": archive_item.mime_type,
                    "file_size": archive_item.file_size,
                    "detection_info": detection_result
                })
                
                logger.info(f"âœ… Uploaded file: {file.filename} -> {safe_filename}")
                
            except Exception as e:
                logger.error(f"âŒ Failed to upload file {file.filename}: {e}")
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to upload file {file.filename}"
                )
        
        await db.commit()
        
        # SECURITY: Move all temp files to final locations ONLY after successful DB commit
        for temp_path, final_path in temp_file_paths:
            try:
                if temp_path.exists():
                    temp_path.rename(final_path)
                    logger.info(f"âœ… File moved to final location: {final_path}")
                else:
                    logger.warning(f"âš ï¸ Temp file not found: {temp_path}")
            except Exception as move_error:
                logger.error(f"âŒ Failed to move file to final location: {move_error}")
                # Clean up temp file
                try:
                    if temp_path.exists():
                        temp_path.unlink()
                except Exception:
                    pass
        
        logger.info(f"ðŸŽ‰ Successfully uploaded {len(uploaded_files)} files")
        
        return {
            "message": f"Successfully uploaded {len(uploaded_files)} files",
            "files": uploaded_files,
            "total_files": len(uploaded_files)
        }
        
    except HTTPException:
        await db.rollback()
        # Clean up all temp files on HTTP exception
        for temp_path, final_path in temp_file_paths:
            try:
                if temp_path.exists():
                    temp_path.unlink()
            except Exception:
                pass
        raise
    except Exception as e:
        await db.rollback()
        # Clean up all temp files on DB rollback
        for temp_path, final_path in temp_file_paths:
            try:
                if temp_path.exists():
                    temp_path.unlink()
            except Exception:
                pass
        logger.error(f"âŒ Archive upload failed: {e}")
        raise HTTPException(status_code=500, detail="Upload failed") 



@router.post("/upload/commit", response_model=ItemResponse)
async def commit_uploaded_file(
    payload: CommitUploadRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Finalize a previously chunk-uploaded file: move it to the archive folder and create DB record."""
    try:
        # Validate payload
        if not payload.file_id:
            raise HTTPException(status_code=400, detail="file_id is required")
        if not payload.folder_uuid:
            raise HTTPException(status_code=400, detail="folder_uuid is required")
        
        # Check assembled file status
        status = await chunk_manager.get_upload_status(payload.file_id)
        if not status or status.get("status") != "completed":
            raise HTTPException(status_code=400, detail="File not yet assembled or assembly failed")

        # Locate assembled file path
        temp_dir = Path(get_data_dir()) / "temp_uploads"
        assembled = next(temp_dir.glob(f"complete_{payload.file_id}_*"), None)
        if not assembled or not assembled.exists():
            raise HTTPException(status_code=404, detail="Assembled file not found on disk")

        # Validate destination folder exists and user has access
        folder_res = await db.execute(
            select(ArchiveFolder).where(
                and_(
                    ArchiveFolder.uuid == payload.folder_uuid,
                    ArchiveFolder.user_id == current_user.id
                )
            )
        )
        folder = folder_res.scalar_one_or_none()
        if not folder:
            raise HTTPException(status_code=404, detail="Target folder not found or access denied")

        # Validate file size
        assembled_file_size = assembled.stat().st_size
        try:
            validate_file_size(assembled_file_size, MAX_FILE_SIZE)
        except Exception as e:
            # Clean up assembled file
            try:
                assembled.unlink()
            except Exception:
                pass
            raise HTTPException(status_code=413, detail=str(e))

        # Prepare destination directory using UUID-based path hierarchy
        filesystem_path = await get_filesystem_path(folder.uuid, db)
        dest_dir = Path(get_file_storage_dir()) / "archive" / filesystem_path.strip("/")
        dest_dir.mkdir(parents=True, exist_ok=True)

        stored_filename = f"{uuid_lib.uuid4()}{assembled.suffix}"
        dest_path = dest_dir / stored_filename
        
        # Move file with robust error handling for Windows file locking
        original_size = assembled.stat().st_size
        try:
            # On Windows, rename can fail if destination exists and is locked
            if dest_path.exists():
                # Try to remove existing file first (Windows-specific issue)
                try:
                    dest_path.unlink()
                except PermissionError:
                    # File is locked, try alternative approach
                    import shutil
                    shutil.move(str(assembled), str(dest_path))
                else:
                    # File removed successfully, now rename
                    assembled.rename(dest_path)
            else:
                # Destination doesn't exist, safe to rename
                assembled.rename(dest_path)
            
            # SECURITY: Verify file integrity after move
            if not dest_path.exists():
                raise Exception("Destination file does not exist after move")
            
            moved_size = dest_path.stat().st_size
            if moved_size != original_size:
                # File size mismatch - move may have corrupted the file
                logger.error(f"âŒ File size mismatch after move: original={original_size}, moved={moved_size}")
                # Clean up corrupted file
                try:
                    dest_path.unlink()
                except Exception:
                    pass
                raise Exception(f"File integrity check failed: size mismatch ({original_size} vs {moved_size})")
            
            logger.info(f"âœ… File moved successfully with integrity verified: {original_size} bytes")
            
        except Exception as e:
            logger.error(f"âŒ Failed to move assembled file: {str(e)}")
            # Clean up assembled file on failure
            try:
                if assembled.exists():
                    assembled.unlink()
            except Exception:
                pass
            raise HTTPException(status_code=500, detail="Failed to move file to archive location")

        # Get final file size for database operations  
        final_file_size = dest_path.stat().st_size
        
        # Create archive item using shared helper
        item = await _create_archive_item(
            db=db,
            file_path=dest_path,
            folder_uuid=payload.folder_uuid,
            original_filename=assembled.name,
            stored_filename=stored_filename,
            mime_type=status.get("mime_type", "application/octet-stream"),
            file_size=final_file_size,
            user_id=current_user.id,
            name=payload.name,
            description=payload.description,
            tags=payload.tags
        )
        
        await db.commit()
        await db.refresh(item)

        # Remove tracking from chunk manager
        if payload.file_id in chunk_manager.uploads:
            del chunk_manager.uploads[payload.file_id]

        logger.info(f"âœ… Committed chunked upload '{item.original_filename}' to folder '{folder.name}' for user {current_user.username}")
        
        return await _get_item_with_relations(db, item.uuid)
        
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        await db.rollback()
        raise
    except Exception as e:
        logger.error(f"âŒ Error committing chunked upload: {str(e)}")
        await db.rollback()
        
        # Clean up destination file if it was created
        try:
            if 'dest_path' in locals() and dest_path.exists():
                dest_path.unlink()
        except Exception as cleanup_error:
            logger.error(f"âŒ Failed to cleanup destination file: {str(cleanup_error)}")
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to commit chunked upload. Please try again."
        )

# Test endpoint for FTS integration debugging
@router.get("/debug/fts-status")
async def debug_fts_status(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Debug endpoint to check FTS integration status for archive items"""
    try:
        # Check if FTS tables exist and have data
        result = await db.execute(text("""
            SELECT COUNT(*) as archive_items_count 
            FROM archive_items 
            WHERE user_id = :user_id
        """), {"user_id": current_user.id})
        archive_items_count = result.scalar()
        
        # Check FTS table
        try:
            fts_result = await db.execute(text("""
                SELECT COUNT(*) as fts_count 
                FROM fts_archive_items 
                WHERE user_id = :user_id
            """), {"user_id": current_user.id})
            fts_count = fts_result.scalar()
        except Exception as e:
            fts_count = f"Error: {str(e)}"
        
        # Test a simple search
        try:
            search_result = await enhanced_fts_service.search_archive_items(db, "test", current_user.id, limit=5)
            search_working = True
            search_results_count = len(search_result)
        except Exception as e:
            search_working = False
            search_results_count = f"Error: {str(e)}"
        
        return {
            "archive_items_count": archive_items_count,
            "fts_items_count": fts_count,
            "fts_search_working": search_working,
            "test_search_results": search_results_count,
            "fts_tables_initialized": enhanced_fts_service.tables_initialized
        }
        
    except Exception as e:
        logger.error(f"âŒ FTS debug check failed: {str(e)}")
        return {
            "error": str(e),
            "archive_items_count": 0,
            "fts_items_count": 0,
            "fts_search_working": False,
            "test_search_results": 0,
            "fts_tables_initialized": False
        }


# Additional endpoints for file/folder management

@router.patch("/folders/{folder_uuid}/rename")
async def rename_folder(
    folder_uuid: str,
    request: Request,
    new_name: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Rename a folder"""
    try:
        # Get the folder
        result = await db.execute(
            select(ArchiveFolder).where(
                and_(
                    ArchiveFolder.uuid == folder_uuid,
                    ArchiveFolder.user_id == current_user.id
                )
            )
        )
        folder = result.scalar_one_or_none()
        
        if not folder:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Folder not found"
            )
        
        # Validate new folder name is unique at this level
        await validate_folder_name(new_name, folder.parent_uuid, current_user.id, db, exclude_uuid=folder.uuid)
        
        # Update the folder name
        folder.name = new_name.strip()
        folder.updated_at = datetime.utcnow()
        
        await db.commit()
        await db.refresh(folder)
        
        return {"success": True, "message": "Folder renamed successfully"}
        
    except Exception as e:
        await db.rollback()
        logger.error(f"Error renaming folder {folder_uuid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to rename folder"
        )


@router.patch("/items/{item_uuid}/rename")
async def rename_item(
    item_uuid: str,
    request: Request,
    new_name: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Rename a file/item"""
    try:
        # Get the item
        result = await db.execute(
            select(ArchiveItem).where(
                and_(
                    ArchiveItem.uuid == item_uuid,
                    ArchiveItem.user_id == current_user.id
                )
            )
        )
        item = result.scalar_one_or_none()
        
        if not item:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Item not found"
            )
        
        # Update the item name
        item.name = new_name.strip()
        item.updated_at = datetime.utcnow()
        
        await db.commit()
        await db.refresh(item)
        
        return {"success": True, "message": "Item renamed successfully"}
        
    except Exception as e:
        await db.rollback()
        logger.error(f"Error renaming item {item_uuid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to rename item"
        )



@router.get("/folders/{folder_uuid}/download")
async def download_folder(
    folder_uuid: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Download a folder as a ZIP file"""
    try:
        # Get the folder
        result = await db.execute(
            select(ArchiveFolder).where(
                and_(
                    ArchiveFolder.uuid == folder_uuid,
                    ArchiveFolder.user_id == current_user.id
                )
            )
        )
        folder = result.scalar_one_or_none()
        
        if not folder:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Folder not found"
            )
        
        # For now, return a placeholder response
        # TODO: Implement actual ZIP creation
        return {"success": True, "message": "Folder download functionality coming soon"}
        
    except Exception as e:
        logger.error(f"Error downloading folder {folder_uuid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to download folder"
        )